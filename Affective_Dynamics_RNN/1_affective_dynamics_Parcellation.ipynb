{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Create parcellation for RNN models:\n",
    "\n",
    "- Input: Denoised timeseries data (nii.gz)\n",
    "- Output: one csv with parcellations as columns, and time points as rows, per subject per run\n",
    "- Behavioral data are added for convenience later\n",
    "- Atlas: schaffer 2018 17network or 7network\n",
    "- N_ROI: 100\n",
    "\n",
    "*Yiyu Wang 2022 November*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import nibabel as nib\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Parcellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '103', '104', '105', '106', '107', '108', '110', '111', '112', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '132', '133', '134', '135', '136', '137', '139', '140', '142', '143', '144', '145', '146', '147', '149', '150', '151', '152', '153', '154', '157', '158', '159', '160', '161', '162', '163', '164', '166', '169', '170', '171', '172', '174', '176', '177', '179', '180', '181', '182', '183', '184', '185']\n",
      "total subs = 70\n"
     ]
    }
   ],
   "source": [
    "# parcellate:\n",
    "make_parcellation = True\n",
    "\n",
    "# which task data to load:\n",
    "task = 'Mem'\n",
    "\n",
    "if task == 'Novel':\n",
    "    included_data = pd.read_csv('/work/abslab/AVFP/Preproc_Scripts/included_AVFP_novel_subjects.csv', header=None)\n",
    "    run_list = [3,4,5]\n",
    "elif task == 'Mem':\n",
    "    included_data = pd.read_csv('/work/abslab/AVFP/Preproc_Scripts/included_AVFP_memory_subjects.csv', header=None)\n",
    "    run_list = [1,2]\n",
    "    \n",
    "subIDs = included_data[0].astype('str').tolist()\n",
    "\n",
    "\n",
    "data_dir = '/work/abslab/Yiyu/AVFP_analysis/denoised_visreg/'\n",
    "\n",
    "print(subIDs)\n",
    "total_subs = len(subIDs)\n",
    "print(f\"total subs = {total_subs}\")\n",
    "\n",
    "TR_LENGTH = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang.yiyu/.local/lib/python3.7/site-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# create whole brain atlas and labels\n",
    "\n",
    "from nilearn import plotting, image, input_data\n",
    "N_ROI = 100\n",
    "which_atlas = 'network7'\n",
    "\n",
    "if which_atlas == 'network17':\n",
    "    # make parcellation masker from pre-saved atlas nifti and atlas labels tsv:\n",
    "    roi_dir = '/work/abslab/Yiyu/masks/'\n",
    "\n",
    "    atlas_img = image.load_img(roi_dir + f'tpl-MNI152NLin6Asym_res-02_atlas-Schaefer2018_desc-{N_ROI}Parcels17Networks_dseg.nii.gz')\n",
    "    atlas_data = image.get_data(atlas_img)\n",
    "\n",
    "    atlas_info = pd.read_csv(roi_dir + f'tpl-MNI152NLin6Asym_atlas-Schaefer2018_desc-{N_ROI}Parcels17Networks_dseg.tsv', delimiter='\\t')\n",
    "    atlas_info.set_index('index', inplace=True)\n",
    "    my_atlas = atlas_info.reset_index().rename(columns={'index':'new_label'})\n",
    "    wholebrain_data = atlas_data\n",
    "    max_label = my_atlas.new_label.max()\n",
    "    wholebrain_img = image.new_img_like(atlas_img, wholebrain_data, affine=atlas_img.affine)\n",
    "    wholebrain_img.to_filename(f'/work/abslab/Yiyu/dnn/masks/wholebrain_atlas_{N_ROI}.nii.gz')\n",
    "\n",
    "    my_atlas[['name','new_label']].to_csv(f'/work/abslab/Yiyu/dnn/masks/wholebrain_atlas-{which_atlas}_{N_ROI}.csv', index=False, header=True)\n",
    "\n",
    "    masker = input_data.NiftiLabelsMasker(\n",
    "        labels_img=labels_img=wholebrain_img,\n",
    "        detrend=True,\n",
    "        standardize=True,\n",
    "        verbose=1)\n",
    "\n",
    "    # plot:\n",
    "    plotting.view_img_on_surf(wholebrain_img, threshold=1,\n",
    "                          vol_to_surf_kwargs={\"n_samples\": 1, \"radius\": 0.0,\n",
    "                                              \"interpolation\": \"nearest\"},\n",
    "                          title='ROIs', vmax = np.max(wholebrain_data),\n",
    "                          symmetric_cmap=False, cmap=\"gist_ncar\")\n",
    "\n",
    "elif which_atlas == 'network7':\n",
    "    from nilearn import datasets\n",
    "    atlas = datasets.fetch_atlas_schaefer_2018(n_rois=N_ROI,yeo_networks=7, resolution_mm=2)\n",
    "    masker = input_data.NiftiLabelsMasker(\n",
    "        labels_img=atlas['maps'],\n",
    "        labels=atlas['labels'],\n",
    "        detrend=True,\n",
    "        standardize=True,\n",
    "        memory='nilearn_cache',\n",
    "        verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch onset files:\n",
    "def GetSubjectOnset(this_sub, run, task):\n",
    "    if task == 'Novel':\n",
    "        logfiles_dir = '/work/abslab/AVFP/logfiles/AffVidsNovel_logfiles'\n",
    "    elif task == 'Mem':\n",
    "        logfiles_dir = '/work/abslab/AVFP/logfiles/AffVidsMem_logfiles'    \n",
    "    onset_files = glob.glob(logfiles_dir + \"/*.txt\")        \n",
    "    # column names of onset files:\n",
    "    col_names = ['video_name','video_number','video_category','novel_familiar','run_number',\n",
    "                 'video_onset','video_offset','video_duration_method1','video_duration_method2',\n",
    "                 'fear_rating_onset','fear_rating','fear_rating_RT',\n",
    "                 'arousal_rating_onset','arousal_rating','arousal_rating_RT',\n",
    "                 'valence_rating_onset','valence_rating','valence_rating_RT']  \n",
    "    # Load onsets, for both runs:\n",
    "    onset_file = [i for i in onset_files if 'sub_' + this_sub in i][0]\n",
    "    onset_data = pd.read_csv(onset_file, delimiter=' ', header=None)\n",
    "    onset_data = onset_data.iloc[:,0:18] #remove extra cols\n",
    "    onset_data.columns=col_names\n",
    "    onset_data['subID'] = this_sub\n",
    "        \n",
    "    onset_data = onset_data.loc[onset_data.run_number==run]\n",
    "    \n",
    "    \n",
    "    # remove nan from ratings:\n",
    "    return onset_data\n",
    " \n",
    "    \n",
    "# concatenate the subject information to parcellation df:\n",
    "def AddSubjectInfoToDf(parcellated_data_df, run_df, hemo_dynamic_lag = 5):\n",
    "    #parcellated_data_df[['video_name','video_category','fear','valence','arousal']]=0\n",
    "    parcellated_data_df = parcellated_data_df.reindex(columns=[*parcellated_data_df.columns.tolist(), 'video_name','video_category','fear','valence','arousal'], fill_value=0)\n",
    "    \n",
    "    for trial_idx,trial_df in run_df.iterrows():\n",
    "\n",
    "        start_tr = int(trial_df.video_onset/TR_LENGTH) + hemo_dynamic_lag\n",
    "        video_duration = 20 # using fix durations instead of using trial_df.video_duration_method2 for RNN\n",
    "        video_duration_in_trs = int(video_duration/TR_LENGTH)\n",
    "        end_tr = start_tr + video_duration_in_trs\n",
    "        parcellated_data_df.iloc[start_tr:end_tr,-5]=trial_df.video_name\n",
    "        parcellated_data_df.iloc[start_tr:end_tr,-4]=trial_df.video_category\n",
    "        parcellated_data_df.iloc[start_tr:end_tr,-3]=trial_df.fear_rating\n",
    "        parcellated_data_df.iloc[start_tr:end_tr,-2]=trial_df.valence_rating\n",
    "        parcellated_data_df.iloc[start_tr:end_tr,-1]=trial_df.arousal_rating\n",
    "    \n",
    "    return parcellated_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "110\n",
      "111\n",
      "112\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "139\n",
      "140\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "166\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "174\n",
      "176\n",
      "177\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n"
     ]
    }
   ],
   "source": [
    "# parcellate: \n",
    "if make_parcellation:\n",
    "    save_dir = f'/work/abslab/Yiyu/dnn/AVFP_parcellation/wholebrain_schaeffer_atlas-{which_atlas}_{N_ROI}/'\n",
    "    for subject in subIDs:\n",
    "        print(subject)\n",
    "        for run in run_list:\n",
    "            file_name = data_dir + f'{subject}/sub-{subject}_run-{run}_AVFP_denoised_novideoregs.nii.gz'\n",
    "            denoised_img =  nib.load(file_name)\n",
    "            parcellated_data = masker.fit_transform(denoised_img)\n",
    "            \n",
    "            parcellated_data_df = pd.DataFrame(parcellated_data,columns=atlas_info['name'])\n",
    "            \n",
    "            # add behavioral information to all TRs, adjusted for HRF: video name, fear rating, valence rating, arousal rating\n",
    "            run_df = GetSubjectOnset(subject, run, task)\n",
    "            parcellated_data_df = AddSubjectInfoToDf(parcellated_data_df, run_df)\n",
    "            \n",
    "            parcellated_data_df.to_csv(save_dir+f'/par-{subject}_run-{run}_schaefer_{N_ROI}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a quick visual\n",
    "parcelatted_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
