{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. run RNN on affvids: \n",
    "\n",
    "*Yiyu Wang 2022 October*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "HTFATorch                /home/wang.yiyu/.conda/envs/HTFATorch\n",
      "NTFA_env3                /home/wang.yiyu/.conda/envs/NTFA_env3\n",
      "base                  *  /work/abslab/Yiyu/DNN_env\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import string\n",
    "import collections\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "SEED = 2022\n",
    "N_ROI = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the subjects:\n",
    "included_data = pd.read_csv('/work/abslab/AVFP/Preproc_Scripts/included_AVFP_novel_subjects.csv', header=None)\n",
    "subIDs = included_data[0].astype('str').tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomAffVidsDynamicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def GetDataName(subject, run='*'):\n",
    "    file_name = glob.glob(f'/work/abslab/Yiyu/dnn/AVFP_parcellation/wholebrain_schaeffer_{N_ROI}/par-{subject}_run-{run}_schaefer_{N_ROI}.csv')\n",
    "    if not file_name :\n",
    "        raise Exception('No this parcellation csv file!')\n",
    "    return file_name\n",
    "\n",
    "def CreateXY(sub_list, network = None, y_col='fear'):\n",
    "    \"\"\"\n",
    "    Run gradient descent to opimize parameters of a given network\n",
    "\n",
    "    Args:\n",
    "    sub_list: list\n",
    "        list of subject IDs to extract the data\n",
    "    network: str \n",
    "        which network to lesion, or default is no lesion \n",
    "        the network name must be in the dataframe column names\n",
    "    y_col: str\n",
    "        which column to extract the y variable\n",
    "        default = 'fear'\n",
    "\n",
    "    Returns:\n",
    "    X_tensor: Tensor\n",
    "        batch x seq x feature\n",
    "    Y_tensor: Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for sub in sub_list:\n",
    "        for parcellation_path in GetDataName(sub):\n",
    "            par_df =pd.read_csv(parcellation_path)\n",
    "            par_df = par_df.loc[par_df['video_name']!='0']\n",
    "            \n",
    "            if network:\n",
    "                x_cols = [col for col in par_df.columns if 'Networks' in col]\n",
    "                zero_cols = [col for col in x_cols if (network in col)]\n",
    "                par_df[zero_cols]=0\n",
    "                  \n",
    "            avg = par_df[y_col].mean()\n",
    "            std = par_df[y_col].std()\n",
    "            for vid_name, trial_df in par_df.groupby('video_name'):\n",
    "                if ~np.isnan(trial_df[y_col].unique()[0]):\n",
    "                    X.append(trial_df.iloc[:,0:N_ROI].astype(float).values)\n",
    "                    Y.append((trial_df[y_col].unique()[0]-avg)/std)\n",
    "\n",
    "    # concate the x and y\n",
    "    # x: batch x seq  x feature\n",
    "    X_tensor = torch.tensor(np.array(X))\n",
    "    # .permute(2, 1, 0)\n",
    "    Y_tensor = torch.tensor(Y)\n",
    "    return X_tensor, Y_tensor\n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the RNN model (LSTM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainLTSM_Classifier(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, feature_dim, n_layers):\n",
    "        super(BrainLTSM_Classifier, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.feature_dim = feature_dim #input dim\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.init_linear = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(feature_dim, hidden_size, n_layers, batch_first=True,dropout=.2, bidirectional=True)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_size * 2, output_size) #bidirectional: hidden_size * 2 (for backward processing)\n",
    "        # self.linear = nn.Linear(self.hidden_size, output_size) #unidirectional\n",
    "      \n",
    "\n",
    "    def init_hidden(self):\n",
    "        # batch = len(inputs) (if batch_first = True)\n",
    "        return (torch.zeros(self.n_layers, self.batch_size, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, self.batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Forward pass through initial hidden layer\n",
    "        linear_input = self.init_linear(input)\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both\n",
    "        # have shape (batch_size, num_layers, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(linear_input)\n",
    "\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        return y_pred, self.hidden    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training functions:\n",
    "\n",
    "def GetMaxAcc(max_acc, current_acc):\n",
    "    save_state = False\n",
    "    if abs(current_acc) > abs(max_acc):\n",
    "        max_acc = current_acc\n",
    "        save_state = True\n",
    "    return max_acc, save_state\n",
    "\n",
    "def EpochTrainTest(model, loss_fn, train_loader, test_loader,\n",
    "          n_epochs, optimizer, device='cpu', verbose=False):\n",
    "    \"\"\"\n",
    "    Run gradient descent to opimize parameters of a given model\n",
    "\n",
    "    Args:\n",
    "    model: nn.Module\n",
    "      PyTorch network whose parameters to optimize\n",
    "    loss_fn:\n",
    "      loss function to minimize\n",
    "    train_loader: Dataloader object\n",
    "    test_loader: Dataloader object\n",
    "    n_epoch: Integer\n",
    "      number of epochs\n",
    "    optimizer: \n",
    "      built-in optimizer from torch\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    train_loss/test_loss: List\n",
    "      Training/Test loss over epochs\n",
    "    \"\"\"\n",
    "\n",
    "    # Placeholder to save the loss at each iteration\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    max_acc = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_train_loss = []\n",
    "        epoch_train_accuracy = []\n",
    "        \n",
    "        # Train:\n",
    "        model.train()\n",
    "        for X, Y in tqdm(train_dataloader):\n",
    "            X.to(device)\n",
    "            Y.to(device)\n",
    "\n",
    "            Y_preds, hidden = model(X.float())\n",
    "\n",
    "            this_loss = loss_fn(Y_preds[:,-1,0], Y.float())\n",
    "            epoch_train_loss.append(this_loss.item())\n",
    "            # Clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Compute gradients\n",
    "            this_loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            this_acc= np.corrcoef(Y.detach().numpy(),\n",
    "                                      Y_preds.detach().squeeze().numpy()[:,-1])[0,1]\n",
    "            epoch_train_accuracy.append(this_acc)\n",
    "            \n",
    "        train_loss.append(torch.tensor(epoch_train_loss).mean())\n",
    "        train_acc.append(torch.tensor(epoch_train_accuracy).mean())\n",
    "         \n",
    "        # Test:\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            Y_shuffled, Y_preds = [],[]\n",
    "            for X, Y in test_dataloader:\n",
    "                X.to(device),Y.to(device)\n",
    "                preds, hidden = model(X.float())\n",
    "                epoch_test_loss = loss_fn(preds[:,-1,0], Y.float())\n",
    "                \n",
    "                test_loss.append(epoch_test_loss.item())\n",
    "                Y_shuffled.append(Y)\n",
    "                Y_preds.append(preds)\n",
    "            Y_shuffled = torch.cat(Y_shuffled)\n",
    "            Y_preds = torch.cat(Y_preds)\n",
    "            epoch_test_accuracy = np.corrcoef(Y_shuffled.detach().numpy(),\n",
    "                                          Y_preds.detach().squeeze().numpy()[:,-1])[0,1]  \n",
    "            \n",
    "            test_acc.append(epoch_test_accuracy)\n",
    "            max_acc, save_state = GetMaxAcc(max_acc, epoch_test_accuracy)\n",
    "            if save_state:\n",
    "                model_state = model.state_dict()\n",
    "                \n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Train Loss : {:.3f}\".format(torch.tensor(epoch_train_loss).mean()))   \n",
    "            print(\"Train accuracy {:.3f}\".format(torch.tensor(epoch_train_accuracy).mean()))\n",
    "            print(f'iteration {epoch + 1}/{n_epochs} | train loss: {np.mean(epoch_train_loss):.3f} | train acc: {np.mean(epoch_train_accuracy):.3f}')\n",
    "            print(f'iteration {epoch + 1}/{n_epochs} | test loss: {epoch_test_loss:.3f} | test acc: {epoch_test_accuracy:.3f}')\n",
    "\n",
    "  \n",
    "    print('maximum accuracy: {:.3f}'.format(max_acc))\n",
    "    print('mean accuracy: {:.3f}'.format(np.mean(test_acc)))\n",
    "    \n",
    "    \n",
    "\n",
    "    return  max_acc, model_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- fold 1 ---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:05<00:00, 17.48it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.40it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.47it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.51it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.64it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.58it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.62it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.64it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.62it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum accuracy: 0.364\n",
      "mean accuracy: 0.351\n",
      "saving: models/LSTM_feature-100_hidden-150_layers-2_fold-0_lr-0.001.pt\n",
      "---------------------- fold 2 ---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:05<00:00, 17.46it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.23it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.39it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.26it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.17it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.24it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.24it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.30it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.34it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum accuracy: 0.409\n",
      "mean accuracy: 0.381\n",
      "saving: models/LSTM_feature-100_hidden-150_layers-2_fold-1_lr-0.001.pt\n",
      "---------------------- fold 3 ---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:05<00:00, 17.22it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.19it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.16it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.45it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.26it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.35it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.29it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.27it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.49it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum accuracy: 0.394\n",
      "mean accuracy: 0.364\n",
      "saving: models/LSTM_feature-100_hidden-150_layers-2_fold-2_lr-0.001.pt\n",
      "---------------------- fold 4 ---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:06<00:00, 16.68it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.65it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.68it/s]\n",
      "100%|██████████| 101/101 [00:06<00:00, 16.74it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.18it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.27it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.19it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.27it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.26it/s]\n",
      "100%|██████████| 101/101 [00:05<00:00, 17.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum accuracy: 0.288\n",
      "mean accuracy: 0.269\n",
      "saving: models/LSTM_feature-100_hidden-150_layers-2_fold-3_lr-0.001.pt\n",
      "---------------------- fold 5 ---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:05<00:00, 17.28it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.13it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.08it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.02it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.11it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.10it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.15it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.17it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.11it/s]\n",
      "100%|██████████| 102/102 [00:05<00:00, 17.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum accuracy: 0.353\n",
      "mean accuracy: 0.319\n",
      "saving: models/LSTM_feature-100_hidden-150_layers-2_fold-4_lr-0.001.pt\n",
      "\n",
      "\n",
      " mean acc 0.36152574801743204\n"
     ]
    }
   ],
   "source": [
    "# run kfold on subjects:\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = SEED)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "max_acc_list = []\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# model params:\n",
    "output_size = 1\n",
    "hidden_size = 150\n",
    "feature_dim = N_ROI\n",
    "n_layers=2\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(subIDs)):\n",
    "    print(f'---------------------- fold {i+1} ---------------------------')\n",
    "    \n",
    "    sub_train = np.array(subIDs)[train_idx.astype(int)].tolist()\n",
    "    sub_test = np.array(subIDs)[test_idx.astype(int)].tolist()\n",
    "    \n",
    "    train_X, train_Y = CreateXY(sub_train)\n",
    "    test_X, test_Y = CreateXY(sub_test)\n",
    "\n",
    "    train_dataset = CustomAffVidsDynamicDataset(train_X, train_Y)\n",
    "    test_dataset = CustomAffVidsDynamicDataset(test_X, test_Y)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "    \n",
    "    \n",
    "    model = BrainLTSM_Classifier(batch_size, output_size, hidden_size, feature_dim, n_layers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    max_acc_fold, model_state = EpochTrainTest(model, loss_fn, train_dataloader, test_dataloader, epochs, optimizer, device)\n",
    "    max_acc_list.append(max_acc_fold)\n",
    "    \n",
    "    # save the model_sate\n",
    "    model_file_path = f'models/LSTM_feature-{feature_dim}_hidden-{hidden_size}_layers-{n_layers}_fold-{i}_lr-{learning_rate}.pt'\n",
    "    print(f'saving: {model_file_path}')\n",
    "    torch.save(model_state, model_file_path)\n",
    "    \n",
    "print('\\n\\n mean acc',np.mean(max_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
